# Daily Kafka Questions Database
# Categories: core-concepts, troubleshooting, configuration
# Difficulty: beginner, intermediate, advanced

# Core Concepts (10 questions)
- id: 1
  category: "core-concepts"
  difficulty: "beginner"
  question: "What is the default retention period for Kafka topics?"
  options:
    - "24 hours"
    - "7 days"
    - "30 days"
    - "Forever"
  correct: 1
  explanation: "The default retention period is 7 days (168 hours), configured via log.retention.hours. After this period, messages are eligible for deletion."
  docs_link: "https://kafka.apache.org/documentation/#brokerconfigs_log.retention.hours"

- id: 2
  category: "core-concepts"
  difficulty: "beginner"
  question: "What uniquely identifies a message within a Kafka partition?"
  options:
    - "Message ID"
    - "Offset"
    - "Timestamp"
    - "Key hash"
  correct: 1
  explanation: "The offset is a sequential, immutable number assigned to each message within a partition. It uniquely identifies the message's position in the partition log."
  docs_link: "https://kafka.apache.org/documentation/#intro_concepts_and_terms"

- id: 3
  category: "core-concepts"
  difficulty: "intermediate"
  question: "What happens when a consumer group has more consumers than partitions?"
  options:
    - "Messages are duplicated across consumers"
    - "Some consumers remain idle"
    - "Kafka throws an error"
    - "Partitions are automatically created"
  correct: 1
  explanation: "Each partition can only be assigned to one consumer within a group. If there are more consumers than partitions, some consumers will be idle and not receive any messages."
  docs_link: "https://kafka.apache.org/documentation/#intro_consumers"

- id: 4
  category: "core-concepts"
  difficulty: "beginner"
  question: "What is the minimum number of in-sync replicas needed for a producer with acks=all to succeed?"
  options:
    - "All replicas"
    - "min.insync.replicas"
    - "Just the leader"
    - "Majority of replicas"
  correct: 1
  explanation: "With acks=all, the producer waits for acknowledgment from all in-sync replicas (ISR). The min.insync.replicas setting defines the minimum ISR count required for writes to succeed."
  docs_link: "https://kafka.apache.org/documentation/#brokerconfigs_min.insync.replicas"

- id: 5
  category: "core-concepts"
  difficulty: "intermediate"
  question: "How does Kafka guarantee message ordering?"
  options:
    - "Globally across all partitions"
    - "Within a single partition only"
    - "Based on message timestamps"
    - "Using distributed locks"
  correct: 1
  explanation: "Kafka only guarantees message ordering within a single partition. Messages with the same key are sent to the same partition, ensuring ordering for related messages."
  docs_link: "https://kafka.apache.org/documentation/#semantics"

- id: 6
  category: "core-concepts"
  difficulty: "beginner"
  question: "What is a Kafka consumer group?"
  options:
    - "A set of topics consumed together"
    - "Multiple consumers sharing partition assignments"
    - "Brokers that serve the same topic"
    - "A backup of consumer offsets"
  correct: 1
  explanation: "A consumer group is a set of consumers that work together to consume messages from a topic. Partitions are distributed among group members for parallel processing."
  docs_link: "https://kafka.apache.org/documentation/#intro_consumers"

- id: 7
  category: "core-concepts"
  difficulty: "advanced"
  question: "What is the role of the Controller in a Kafka cluster?"
  options:
    - "Handle all client requests"
    - "Manage partition leadership and cluster metadata"
    - "Store consumer offsets"
    - "Compress messages"
  correct: 1
  explanation: "The Controller is a broker elected to manage partition leader elections, handle broker failures, and maintain cluster metadata like partition assignments."
  docs_link: "https://kafka.apache.org/documentation/#design_replicamanagement"

- id: 8
  category: "core-concepts"
  difficulty: "intermediate"
  question: "What is the purpose of the __consumer_offsets topic?"
  options:
    - "Store message data"
    - "Track consumer group committed offsets"
    - "Log broker health metrics"
    - "Store topic configurations"
  correct: 1
  explanation: "The __consumer_offsets internal topic stores committed offsets for all consumer groups. This allows consumers to resume from their last position after restarts."
  docs_link: "https://kafka.apache.org/documentation/#impl_offsettracking"

- id: 9
  category: "core-concepts"
  difficulty: "beginner"
  question: "What is a Kafka broker?"
  options:
    - "A client that produces messages"
    - "A server that stores and serves messages"
    - "A tool for monitoring Kafka"
    - "A message compression algorithm"
  correct: 1
  explanation: "A Kafka broker is a server that receives messages from producers, stores them on disk, and serves them to consumers. A Kafka cluster consists of multiple brokers."
  docs_link: "https://kafka.apache.org/documentation/#introduction"

- id: 10
  category: "core-concepts"
  difficulty: "advanced"
  question: "What is KRaft mode in Kafka?"
  options:
    - "A compression algorithm"
    - "Kafka's built-in consensus without ZooKeeper"
    - "A replication strategy"
    - "A consumer protocol"
  correct: 1
  explanation: "KRaft (Kafka Raft) is Kafka's built-in consensus protocol that eliminates the need for ZooKeeper. It uses the Raft algorithm for metadata management and controller election."
  docs_link: "https://kafka.apache.org/documentation/#kraft"

# Troubleshooting (10 questions)
- id: 11
  category: "troubleshooting"
  difficulty: "intermediate"
  question: "What happens if a Kafka broker goes down?"
  options:
    - "All messages are lost"
    - "Leaders are elected on other brokers for its partitions"
    - "The entire cluster stops"
    - "Producers automatically retry indefinitely"
  correct: 1
  explanation: "When a broker fails, the Controller elects new leaders from the ISR for partitions that had leaders on the failed broker. With proper replication, no data is lost."
  docs_link: "https://kafka.apache.org/documentation/#design_replicamanagement"

- id: 12
  category: "troubleshooting"
  difficulty: "intermediate"
  question: "What is the most common cause of consumer lag?"
  options:
    - "Network latency"
    - "Slow message processing in the consumer"
    - "Too many partitions"
    - "Message compression"
  correct: 1
  explanation: "Consumer lag typically occurs when the consumer's message processing rate is slower than the producer's publish rate. This can be due to slow business logic, external I/O, or insufficient consumer parallelism."
  docs_link: "https://kafka.apache.org/documentation/#consumer_monitoring"

- id: 13
  category: "troubleshooting"
  difficulty: "advanced"
  question: "What causes a 'rebalancing storm' in Kafka consumers?"
  options:
    - "Too many messages"
    - "Consumers timing out during long processing"
    - "Network bandwidth limits"
    - "Disk space issues"
  correct: 1
  explanation: "Rebalancing storms occur when consumers exceed max.poll.interval.ms during processing, causing them to be removed from the group. This triggers rebalancing, which can cascade if the problem persists."
  docs_link: "https://kafka.apache.org/documentation/#consumerconfigs_max.poll.interval.ms"

- id: 14
  category: "troubleshooting"
  difficulty: "intermediate"
  question: "What does 'under-replicated partitions' indicate?"
  options:
    - "Too many consumers"
    - "Some replicas are not in sync with the leader"
    - "Topics have too few partitions"
    - "Message size is too large"
  correct: 1
  explanation: "Under-replicated partitions occur when follower replicas fall behind the leader and are removed from the ISR. This can be caused by broker failures, network issues, or disk problems."
  docs_link: "https://kafka.apache.org/documentation/#basic_ops_cluster_expansion"

- id: 15
  category: "troubleshooting"
  difficulty: "beginner"
  question: "What happens if a consumer crashes mid-processing before committing?"
  options:
    - "Message is lost forever"
    - "Message will be redelivered on next poll"
    - "Other consumers are notified"
    - "Producer resends the message"
  correct: 1
  explanation: "If a consumer crashes before committing its offset, the message will be redelivered to the consumer (or another consumer in the group) on the next poll. This is at-least-once delivery semantics."
  docs_link: "https://kafka.apache.org/documentation/#semantics"

- id: 16
  category: "troubleshooting"
  difficulty: "advanced"
  question: "What can cause 'LEADER_NOT_AVAILABLE' errors?"
  options:
    - "Consumer group is full"
    - "Partition leader election in progress"
    - "Message is too large"
    - "Invalid topic name"
  correct: 1
  explanation: "LEADER_NOT_AVAILABLE occurs when there's no leader for a partition, typically during leader election after a broker failure or when a topic is being created."
  docs_link: "https://kafka.apache.org/documentation/#design_replicamanagement"

- id: 17
  category: "troubleshooting"
  difficulty: "intermediate"
  question: "Why might a producer receive 'RecordTooLargeException'?"
  options:
    - "Too many partitions"
    - "Message exceeds max.message.bytes"
    - "Topic doesn't exist"
    - "Producer buffer is full"
  correct: 1
  explanation: "RecordTooLargeException occurs when a message exceeds the size limits set by message.max.bytes (broker) or max.message.bytes (topic). Default is 1MB."
  docs_link: "https://kafka.apache.org/documentation/#brokerconfigs_message.max.bytes"

- id: 18
  category: "troubleshooting"
  difficulty: "beginner"
  question: "What should you check first when consumers aren't receiving messages?"
  options:
    - "Message compression settings"
    - "Consumer group ID and topic subscription"
    - "Broker disk space"
    - "Producer batch size"
  correct: 1
  explanation: "When consumers aren't receiving messages, first verify the consumer group ID is correct and the consumer is subscribed to the right topics. Also check if offsets are positioned correctly."
  docs_link: "https://kafka.apache.org/documentation/#consumerconfigs"

- id: 19
  category: "troubleshooting"
  difficulty: "advanced"
  question: "What does 'OffsetOutOfRangeException' typically indicate?"
  options:
    - "Consumer lag is too high"
    - "Requested offset no longer exists on broker"
    - "Partition count changed"
    - "Consumer group rebalancing"
  correct: 1
  explanation: "OffsetOutOfRangeException occurs when a consumer requests an offset that no longer exists, usually because messages were deleted due to retention policies. The consumer needs to reset its offset."
  docs_link: "https://kafka.apache.org/documentation/#consumerconfigs_auto.offset.reset"

- id: 20
  category: "troubleshooting"
  difficulty: "intermediate"
  question: "How can you diagnose high producer latency?"
  options:
    - "Check consumer lag"
    - "Monitor batch.size and linger.ms settings"
    - "Increase partition count"
    - "Reduce replication factor"
  correct: 1
  explanation: "High producer latency can be caused by batching settings. If linger.ms is too high, producers wait to fill batches. If batch.size is too large, it takes longer to fill. Also check acks settings and broker response times."
  docs_link: "https://kafka.apache.org/documentation/#producerconfigs"

# Configuration (10 questions)
- id: 21
  category: "configuration"
  difficulty: "beginner"
  question: "What does 'acks=all' mean for a Kafka producer?"
  options:
    - "No acknowledgment needed"
    - "Only leader acknowledgment"
    - "All in-sync replicas must acknowledge"
    - "All brokers must acknowledge"
  correct: 2
  explanation: "With acks=all (or acks=-1), the producer waits for all in-sync replicas to acknowledge the message. This provides the strongest durability guarantee but highest latency."
  docs_link: "https://kafka.apache.org/documentation/#producerconfigs_acks"

- id: 22
  category: "configuration"
  difficulty: "intermediate"
  question: "What is the purpose of 'max.poll.records' consumer config?"
  options:
    - "Maximum messages in a topic"
    - "Maximum messages returned per poll()"
    - "Maximum consumer group size"
    - "Maximum message size"
  correct: 1
  explanation: "max.poll.records limits the number of records returned in a single poll() call. Lower values help prevent consumer timeouts during slow processing."
  docs_link: "https://kafka.apache.org/documentation/#consumerconfigs_max.poll.records"

- id: 23
  category: "configuration"
  difficulty: "intermediate"
  question: "What does 'auto.offset.reset' control?"
  options:
    - "Automatic partition rebalancing"
    - "Where to start reading when no committed offset exists"
    - "Automatic topic creation"
    - "Consumer group cleanup"
  correct: 1
  explanation: "auto.offset.reset determines where the consumer starts reading when there's no initial offset or the current offset no longer exists. Values: 'earliest', 'latest', or 'none'."
  docs_link: "https://kafka.apache.org/documentation/#consumerconfigs_auto.offset.reset"

- id: 24
  category: "configuration"
  difficulty: "advanced"
  question: "What is the recommended 'replication.factor' for production?"
  options:
    - "1"
    - "2"
    - "3"
    - "5"
  correct: 2
  explanation: "A replication factor of 3 is recommended for production. This allows the cluster to tolerate 1 broker failure while maintaining full ISR, and 2 failures while still having data available."
  docs_link: "https://kafka.apache.org/documentation/#replication"

- id: 25
  category: "configuration"
  difficulty: "beginner"
  question: "What is the default number of partitions for a new topic?"
  options:
    - "1"
    - "3"
    - "6"
    - "12"
  correct: 0
  explanation: "By default, new topics are created with 1 partition (num.partitions=1). This should typically be increased based on throughput requirements and consumer parallelism needs."
  docs_link: "https://kafka.apache.org/documentation/#brokerconfigs_num.partitions"

- id: 26
  category: "configuration"
  difficulty: "intermediate"
  question: "What does 'cleanup.policy=compact' do?"
  options:
    - "Compress messages"
    - "Keep only the latest value per key"
    - "Delete old messages faster"
    - "Reduce partition count"
  correct: 1
  explanation: "Log compaction keeps only the most recent value for each message key, effectively providing a changelog where the topic represents the latest state of each key."
  docs_link: "https://kafka.apache.org/documentation/#compaction"

- id: 27
  category: "configuration"
  difficulty: "advanced"
  question: "What is the purpose of 'unclean.leader.election.enable'?"
  options:
    - "Enable automatic topic creation"
    - "Allow out-of-sync replicas to become leaders"
    - "Enable consumer group auto-creation"
    - "Allow producers without authentication"
  correct: 1
  explanation: "When set to true, allows replicas not in ISR to become leaders during failures. This prevents unavailability but risks data loss. Default is false for safety."
  docs_link: "https://kafka.apache.org/documentation/#brokerconfigs_unclean.leader.election.enable"

- id: 28
  category: "configuration"
  difficulty: "intermediate"
  question: "What does 'enable.auto.commit=true' do for consumers?"
  options:
    - "Automatically creates topics"
    - "Periodically commits offsets in the background"
    - "Automatically rebalances partitions"
    - "Auto-scales consumer instances"
  correct: 1
  explanation: "With enable.auto.commit=true, the consumer automatically commits offsets at intervals defined by auto.commit.interval.ms. This simplifies code but may lead to at-least-once processing."
  docs_link: "https://kafka.apache.org/documentation/#consumerconfigs_enable.auto.commit"

- id: 29
  category: "configuration"
  difficulty: "beginner"
  question: "What is the purpose of 'bootstrap.servers' config?"
  options:
    - "Define topic settings"
    - "Initial brokers to connect and discover cluster"
    - "Set consumer starting position"
    - "Configure replication"
  correct: 1
  explanation: "bootstrap.servers is a list of host:port pairs for initial connection. The client uses these to discover the full cluster. Not all brokers need to be listed, just enough for initial contact."
  docs_link: "https://kafka.apache.org/documentation/#producerconfigs_bootstrap.servers"

- id: 30
  category: "configuration"
  difficulty: "advanced"
  question: "What does 'isolation.level=read_committed' mean for consumers?"
  options:
    - "Use SSL for connections"
    - "Only read non-transactional or committed transactional messages"
    - "Isolate consumer from other groups"
    - "Prevent rebalancing"
  correct: 1
  explanation: "With read_committed, consumers only see messages from committed transactions and non-transactional messages. This is essential for exactly-once semantics with transactional producers."
  docs_link: "https://kafka.apache.org/documentation/#consumerconfigs_isolation.level"
